# -*- coding: utf-8 -*-
"""diabetes_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15njebCDX0Z3lXBYSOQc4iUk8NNL5knC6

#  *Predicting* **Diabetes**

*Diabetes is a chronic, metabolic disease characterized by elevated levels of blood glucose (or blood sugar), which leads over time to serious damage to the heart, blood vessels, eyes, kidneys and nerves. The most common is type 2 diabetes, usually in adults, which occurs when the body becomes resistant to insulin or doesn't make enough insulin. In the past three decades the prevalence of type 2 diabetes has risen dramatically in countries of all income levels. Type 1 diabetes, once known as juvenile diabetes or insulin-dependent diabetes, is a chronic condition in which the pancreas produces little or no insulin by itself. For people living with diabetes, access to affordable treatment, including insulin, is critical to their survival.*

**Data Description :-** 

    Pregnancies: Number of times pregnant
    Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
    BloodPressure: Diastolic blood pressure (mm Hg)
    SkinThickness: Triceps skin fold thickness (mm)
    Insulin: 2-Hour serum insulin (mu U/ml)
    BMI: Body mass index (weight in kg/(height in m)^2)
    DiabetesPedigreeFunction: Diabetes pedigree functionr
    Age: Age (years)
    Cabin : Cabin Number
    Outcome: Class variable (0 or 1)

***Importing numpy ,pandas ,matplotlib ,seaborn***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
# %matplotlib inline

"""|from sklearn package   | importing modules|
|-------------   | -----------------|
|preprocessing   | StandardScaler   |
|model_selection | train_test_split, GridSearchCV|
|metrics         | accuracy_score, confusion_matrix|
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import accuracy_score,confusion_matrix

data = pd.read_csv('diabetes.csv')  #import dataset
data.head(10)

data.shape

data.info

"""## **Some stats about data**

"""

data.describe()

"""### **Number of nulls in each column**"""

data.isnull().sum()

"""## **Correlation Matrix**"""

sns.heatmap(data.corr(),cbar=True,cmap='PuOr',annot=False)

"""# **EDA**"""

col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']

"""### Replacing missing data with 0."""

for i in col:
  data[i].replace(0,data[i].mean(),inplace=True)

p=data.hist(figsize = (20,20))

fig, axes = plt.subplots(1, 3,figsize=(15,5))
sns.pointplot(ax= axes[0],x='Outcome', y= 'Age', data=data)
sns.scatterplot(ax = axes[1],x='Age',y='Insulin',data=data)
sns.boxplot(ax = axes[2],x='Outcome',y='Pregnancies',data=data)

fig, axes = plt.subplots(1, 3,figsize=(15,5))
sns.stripplot(ax = axes[0], x='Pregnancies',y='Age',data=data)
sns.regplot(ax = axes[1],x='SkinThickness', y= 'Insulin', data=data)
sns.regplot(ax = axes[2],x='Insulin',y='Age',data=data)

data.var()

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(data.drop(["Outcome"],axis = 1),),
        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])
y=data.Outcome

#splitting the dataset
from sklearn.model_selection import train_test_split        
X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.30,random_state=3)

"""# **Models**"""

#splitting the dataset
from sklearn.model_selection import train_test_split        
X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.30,random_state=3)

"""## 1. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(C=1,penalty='l2')
log_reg.fit(X_train,Y_train)

log_acc=accuracy_score(Y_test,log_reg.predict(X_test))


print("Train Set Accuracy:"+str(accuracy_score(Y_train,log_reg.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,log_reg.predict(X_test))*100))

"""## 2. K-Nearest-Neighbour"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=9)                #knn classifier
knn.fit(X_train,Y_train)

knn_acc = accuracy_score(Y_test,knn.predict(X_test))


print("Train Set Accuracy:"+str(accuracy_score(Y_train,knn.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,knn.predict(X_test))*100))

"""## 3. SVC"""

from sklearn.svm import SVC

svm = SVC()
svm.fit(X_train,Y_train)    

svm_acc= accuracy_score(Y_test,svm.predict(X_test))


print("Train Set Accuracy:"+str(accuracy_score(Y_train,svm.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,svm.predict(X_test))*100))

"""## 4. Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)
dtc.fit(X_train, Y_train)


dtc_acc= accuracy_score(Y_test,dtc.predict(X_test))

print("Train Set Accuracy:"+str(accuracy_score(Y_train,dtc.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,dtc.predict(X_test))*100))

"""## 5. Gradient Boosting Classifier """

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()
gbc.fit(X_train, Y_train)


gbc_acc=accuracy_score(Y_test,gbc.predict(X_test))

print("Train Set Accuracy:"+str(accuracy_score(Y_train,gbc.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,gbc.predict(X_test))*100))

"""## 6. XGBClassifier"""

from xgboost import XGBClassifier

xgb = XGBClassifier(booster = 'gbtree', learning_rate = 0.1, max_depth=6,n_estimators = 10)
xgb.fit(X_train,Y_train)

xgb_acc= accuracy_score(Y_test,xgb.predict(X_test))

print("Train Set Accuracy:"+str(accuracy_score(Y_train,xgb.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(Y_test,xgb.predict(X_test))*100))

"""## 7. Stacking"""

from sklearn.model_selection import train_test_split     #splitting the dataset                                                             
train,val_train,test,val_test = train_test_split(X,y,test_size=.50,random_state=3)

x_train,x_test,y_train,y_test = train_test_split(train,test,test_size=.20,random_state=3)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)

# second model
svm = SVC()
svm.fit(x_train, y_train)

pred_1=knn.predict(val_train)
pred_2=svm.predict(val_train)

# addition of 2 predictions
result = np.column_stack((pred_1,pred_2))

pred_test1=knn.predict(x_test)
pred_test2=svm.predict(x_test)
predict_test=np.column_stack((pred_test1,pred_test2))

# stacking classifier
#RandomForestClasifier:- In this prediction of other 2 classification is taken as x value
from sklearn.ensemble import RandomForestClassifier
rand_clf = RandomForestClassifier()
rand_clf.fit(result,val_test)

rand_clf.score(result,val_test)

rand_acc=accuracy_score(y_test ,rand_clf.predict(predict_test))
rand_acc

models = pd.DataFrame({
    'Model': ['Logistic','KNN', 'SVC',  'Decision Tree Classifier',
             'Gradient Boosting Classifier',  'XgBoost','Stacking'],
    'Score': [ log_acc,knn_acc, svm_acc, dtc_acc, gbc_acc, xgb_acc,rand_acc,]
})

models.sort_values(by = 'Score', ascending = False)

sns.set_color_codes("pastel")
sns.set_style("whitegrid")
plt.figure(figsize=(9,3))
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")
sns.barplot(y=models['Model'],x=models['Score'],color = 'b')
plt.show()